#!/usr/bin/perl -Tw
# from http://www.gluelogic.com/code/BitTorrent/TrackerCGI.pm
package BitTorrent::TrackerCGI;
use BitTorrent::Tracker;

BitTorrent::Tracker::Main();

1;
__END__

Assorted Notes:
---------------

Security: because of no authentication, any peer can contact a tracker and mess
up what the tracker thinks is the state of any downloader.  Also, all statistics
are manipulatable.

This module requires a few RPMs possibly not installed by default on your
Linux distribution.  On RedHat, these might include perl-DBI-*.i386.rpm,
perl-DBD-MySQL-*.i386.rpm, and perl-Digest-SHA1-*.i386.rpm.  Apache::DBI is
also recommended (but not required) for database connection caching, and
usually needs to be added to the system.

HEAP tables are fast and stored in B-trees.  However, their entire contents are
  lost when the database server is stopped (shut down).  The dynamic tables
  keeping track of current downloaders bt_info and bt_data will be completely
  regenerated within the reannounce interval by those downloaders still alive,
  so there is no loss there.  Also, the tables could be regenerated by replaying
  the web log GET requests.
HEAP type tables do not support AUTO_INCREMENT colums; MyISAM tables do.
HEAP tables must have fixed length records.  Resolve and store packed IP address
  to both save space and to avoid a long CHAR() column for ip (not VARCHAR()).
  For trackers that use DNS names that are multi-homed, we only use one address.
  If this is a big deal, maybe the client can check in with multiple addresses?

Similar to the IP address packing, the summary table and names tables are
separate so that the summary table can have a fixed record length.  The summary
table is used and updated much more heavily than the names table, which is only
accessed during refresh and by the scrape interface, and the name column is the
only VARCHAR column between the tables.

peer statistics are kept in bt_data, separate from peer info in bt_info so that
there is less contention between updating the items in the bt_data table on
every hit and choosing random entries from among the items in bt_info.  This
allows the updates to bt_data to be made low priority since the peer for which
the data is being updated probably will not be contacting the server for another
reannounce interval (30 minutes).

The statistics in the summary table may be completely regenerated from the web
server logs, although high traffic sites may wish to disable web logging for the
tracker.  [As an aside, for maximum performance, this code could be modified to
entirely eliminate the summary table, which would also disable the /scrape
interface.  Then, statistics could be generated from the web logs, or even those
can be disabled.]  All that said, for the typical site, bandwidth usage will
probably become a problem well before CPU, memory, or disk usage.

At the moment I am writing this, the bt_data and bt_info tables consume about
172 bytes per entry (total) including indexes were they MyISAM tables.
References:
  http://www.mysql.com/doc/en/Storage_requirements.html
  http://www.mysql.com/doc/en/Key_space.html
For convenience, let's assume the same is true for HEAP tables, which are purely
memory-based B-trees, and let's round up to 192 bytes, so that 16 entries
consume 3 KiB, and 32768 (32 Ki) entries consumes 6 MiB.  32 Ki simultaenous
downloaders is quite a lot.  Not many more than 4 Ki have been seen in the
field, and 4 Ki simultaenous downloaders consumes about 768 KiB of memory for
the HEAP tables, a minuscule amount of memory to require for the db tables on
any serious web server.

Now let's look at some other resource usage.  The default reannounce interval
is 30 minutes, and downloaders can announce themselves more frequently if they
need a new batch of peers.  4 Ki downloaders all announcing themselves once
every 30 minutes leads to an average of 2.3 hits per second, which is not
unreasonable to ask of a web server and database.  Of course, this assumes a
completely even distribution of accouncements from downloaders, which is not
realistic, so one must assume some quiet periods of fewer and some burst periods
of more requests.  In contrast, 32 Ki downloaders would indicate an average of
18.2 hits per second, which might indicate the need for a dedicated machine to
handle the load.

So now that we have established than an average box should be able to handle
4 Ki simultaneous downloaders, let's look at the bandwidth usage, since that is
probably a limiting factor for many, in speed and/or cost.  The typical HTTP
request + bencoded response of 50 peers, including HTTP protocol overhead, is
about 4 KiB.  With 4 Ki downloaders announcing themselves once every 30 minutes
over the course of a day this adds up to 768 MiB of throughput, and over the
course of a month adds up to 22.5 GB.  This assuming constant usage, which does
not happen often, but gives an approximate of the magnitude of what to expect
when running a popular tracker.  In practice, few torrents will ever see 4 Ki
simultaneous downloaders.

Bandwidth usage can be reduced by gzip'ing content to clients that support
gzip Content-encoding.  mod_gzip with Apache 1.3 series and mod_deflate with
Apache 2.0 series are highly recommended.

[Aside:
If you modify this code for us in a public torrent site, it is recommended that
web logs be disabled for privacy reasons.  If someone uploads a torrent for
illegal material -- which your site policy should clearly not allow -- you can
expect your responsibility to be to delete the torrent when informed by
authorities.  If you have web logs in addition, those may be subpoenaed by law
enforcement or by vigilantes like the RIAA or BSA while the authorities
help/turn a blind eye, causing you much time and grief and expense.  YMMV.
Try not to host torrents for any "questionable" material to begin with and you
will be better off.]

The auto-updated timestamp 'mark' column in bt_data is not indexed to avoid the
overhead of updating the index upon every hit.  After all, its value is only
accessed during cleanup (default 15 minutes; twice each reannounce interval),
so cleanup may be a bit slower because it has to run table scans for all the
cleanup queries.  Then again, these are HEAP tables (completely in memory), so
it may not matter much.  If refreshing takes too long, you might want to create
an index on 'mark'.  YMMV.  If you do, might also set PACK_KEYS=1.

There is probably some room to further tune interaction with the database.
We could write our own Apache::DBI for connection and statement caching
(prepare_cached) for additional performance, but more simple to use Apache::DBI
(and more future-proof for threaded Apache2 and mod_perl2) unless you _really_
need to avoid the method call and hash lookup.  For an example, see:
http://take23.org/docs/guide/performance.xml/10


Future possible extensions:
---------------------------
If there is a desire for it, information is already available to allow for a
limit to be placed on the maximum number of concurrent downloaders (peers+scc)
for a torrent, as well as the ratio of peers to scc for a torrent,
e.g. only allow 4*scc < peers+seeds.  To place a maximum across all torrents,
an additional query would need to be made to the database
(SELECT COUNT(*) FROM bt_info), but COUNT(*) queries without columns and without
a WHERE clause and FROM only one table are very fast in MySQL, so such a limit
would be trivial to implement and would have minimal impact as well.


Notes to self:
--------------
Are peer_ids unique between the same client downloading different torrents?
I assumed as such.  If this is not the case, I need to update the PRIMARY KEY
in bt_data table to be (sha1,peer_id) (after adding a sha1 column), and to
update appropriate WHERE clauses in numerous other queries.


See Also:
--------------
http://verysimple.com/2010/01/12/sqlite-lpad-rpad-function/
http://mail-archives.apache.org/mod_mbox/perl-modperl/200509.mbox/%3CPine.LNX.4.63.0509082243310.9521@theoryx5.uwinnipeg.ca%3E
http://www.perturb.org/display/entry/629/
http://www.sqlite.org/lang_insert.html
